<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://kitenote.github.io</id>
    <title>Kite&apos;s blog</title>
    <updated>2020-05-28T10:00:54.454Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://kitenote.github.io"/>
    <link rel="self" href="https://kitenote.github.io/atom.xml"/>
    <subtitle>Kite&apos;s Blog</subtitle>
    <logo>https://kitenote.github.io/images/avatar.png</logo>
    <icon>https://kitenote.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Kite&apos;s blog</rights>
    <entry>
        <title type="html"><![CDATA[The mathematics of optimization for deep learning]]></title>
        <id>https://kitenote.github.io/yWmyCOWDq/</id>
        <link href="https://kitenote.github.io/yWmyCOWDq/">
        </link>
        <updated>2020-05-28T09:14:24.000Z</updated>
        <content type="html"><![CDATA[<h1 id="the-mathematics-of-optimization-for-deep-learning">The mathematics of optimization for deep learning</h1>
<h2 id="a-brief-guide-about-how-to-minimize-a-function-with-millions-of-variables">A brief guide about how to minimize a function with millions of variables</h2>
<p>[</p>
<figure data-type="image" tabindex="1"><img src="assets/1590659436-5798a794cad60926b255be65ee944b22.png" alt="Tivadar Danka" loading="lazy"></figure>
<p>](https://towardsdatascience.com/@tivadar.danka?source=post_page-----11af2b1fda30----------------------)</p>
<p><a href="https://towardsdatascience.com/@tivadar.danka?source=post_page-----11af2b1fda30----------------------">Tivadar Danka</a></p>
<p>Follow</p>
<p><a href="https://towardsdatascience.com/the-mathematics-of-optimization-for-deep-learning-11af2b1fda30?source=post_page-----11af2b1fda30----------------------">Feb 18</a> · 16 min read</p>
<p>In general, the overall performance of a neural network depends on several factors. The one usually taking the spotlight is the network architecture, however, this is only one among many important components. An often overlooked contributor to a performant algorithm is the optimizer, which is used to fit the model.</p>
<p>Just to illustrate the complexity of optimizing, a ResNet18 architecture has 11689512 parameters. Finding an optimal parameter configuration is locating a point in the 11689512 dimensional space. If we were to brute force this, we might decide to divide this space up to a grid, say we select 10 points along each dimension. Then we have to check 10¹¹⁶⁸⁹⁵¹² possible configurations, calculate the loss function for each of them and find the one with minimal loss. To put this number in perspective, there observable universe has about 10⁸³ atoms and it is estimated to be 4.32 x 10¹⁷ seconds (~13.7 billion years) old. If we check as many parameter configuration in each second as the number of atoms starting from the Big Bang, we would have been able to check 4.32 x 10¹⁴¹¹ points until now.</p>
<p>To say that this is not even close is an understatement. The size of the grid is still approximately 10⁸²⁸⁴ times larger than we could check if we would have every atom in the universe check a configuration since the Big Bang.</p>
<p>So, optimizers are pretty important. They are managing this incomprehensible complexity, allowing you to train the neural networks in days instead of billions of years. In the following, we are going to take a deep dive into the mathematics of optimizers and see how are they able to handle this seemingly impossible task.</p>
<h1 id="the-basics-of-optimization">The basics of optimization</h1>
<p>Let’s start simple and suppose that we have a function of one variable which we would like to maximize. (In machine learning context, we generally aim to minimize the loss function, but minimizing is the same as maximizing the negative of the function.) Define</p>
<figure data-type="image" tabindex="2"><img src="assets/1590659436-ead02bf18a81d30bcacb36b702075581.png" alt="" loading="lazy"></figure>
<p>which looks like the following if we plot its graph.</p>
<figure data-type="image" tabindex="3"><img src="assets/1590659436-070fe1466a8a158c4c58a43928fef248.png" alt="" loading="lazy"></figure>
<p>An obvious method to optimize would be to divide up the line into a grid, check the value of every point and select the one where the function is maximized. As we have seen in the introduction, this is not scalable, so we are going to look for another solution. Let’s imagine that this is a mountain landscape and we are climbers, trying to reach the peak. Suppose that we are at the location marked with a red dot.</p>
<figure data-type="image" tabindex="4"><img src="assets/1590659436-9071787be77a96de4b511d4546043b89.png" alt="" loading="lazy"></figure>
<p>If we want to find the peak, which direction should we go? Of course, we should go where the slope is increasing. This concept is formalized by the <em>derivative</em> of a function. Mathematically, the derivative is defined by</p>
<figure data-type="image" tabindex="5"><img src="assets/1590659436-c953eef10de9ce89485b728e33153905.png" alt="" loading="lazy"></figure>
<p>Although this quantity seems mysterious for the first glance, it has a very simple geometric meaning. Let’s look at the function more closely around the point where we take the derivative.</p>
<figure data-type="image" tabindex="6"><img src="assets/1590659436-6df9ba392d75075e52cc981f55875a38.png" alt="" loading="lazy"></figure>
<p>For any <em>x</em> and <em>y</em>, the line passing through <em>f(x)</em> and <em>f(y)</em> is defined by the equation</p>
<figure data-type="image" tabindex="7"><img src="assets/1590659436-40c3fe21e30a82c4a24229784485ecc5.png" alt="" loading="lazy"></figure>
<p>In general, if we have any line defined by <em>at + b</em> for some <em>a</em> and <em>b</em>, the quantity <em>a</em> is called the <em>slope</em> of the line. This can be negative and positive as well, lines with positive slope go upward, while negative ones go downward. Higher values in absolute value mean steeper lines. If we let <em>y</em> closer and closer to <em>x</em> as it is in the definition of derivative, we see that the line becomes the tangent of the function graph at <em>x</em>.</p>
<figure data-type="image" tabindex="8"><img src="assets/1590659436-d6d74e4d002553e640f9df7841796037.png" alt="" loading="lazy"></figure>
<p>Tangent and approximating lines for f(x) at x = -2.0</p>
<p>The tangent is given by the function</p>
<figure data-type="image" tabindex="9"><img src="assets/1590659436-e7fdfa889980cd551737af1d4a6fa641.png" alt="" loading="lazy"></figure>
<p>and its direction can be described with the vector <em>(1, f’(x))</em>.</p>
<p>If we imagine ourselves again into the position of a mountain climber starting from <em>x0 = -2.0</em> , we should go in the direction where the tangent is rising. If the slope of the tangent is large, we would also like to take a large step, while if the slope is close to zero, we should take a smaller step to make sure we don’t go over the peak. To formalize this mathematically, we should go to the next point defined by</p>
<figure data-type="image" tabindex="10"><img src="assets/1590659436-cf9f92e34bce278fb8092ba10aa23e24.png" alt="" loading="lazy"></figure>
<p>where λ is a parameter, setting how large is the step should be in the right direction. This is called the <em>learning rate</em>. In general, subsequent steps are be defined by</p>
<figure data-type="image" tabindex="11"><img src="assets/1590659436-9e6e5cd6570fe703a9cc395ed7041a46.png" alt="" loading="lazy"></figure>
<p>Positive derivative means that the tangent is increasing thus we want to go forward, while negative derivative is a decreasing tangent, so we want to turn back. We can visualize this process.</p>
<figure data-type="image" tabindex="12"><img src="assets/1590659436-7868535c13e5df64a90c98392dd1c1aa.png" alt="" loading="lazy"></figure>
<p>As we can see, this simple algorithm successfully found a peak. However, this is not the global maximum of the function, which can be seen by looking at the image. To get a little ahead of ourselves, this is a potential issue for a wide family of optimizing algorithms, but there are solutions for it.</p>
<p>In this simple case, we have only maximized a function of a single variable. This is useful to illustrate the concept, however, in real-life scenarios, millions of variables can be present. For neural networks, this is definitely the case. In the next part, we are going to see how this simple algorithm can be generalized for optimizing multidimensional functions!</p>
<h1 id="optimizing-in-multiple-dimensions">Optimizing in multiple dimensions</h1>
<p>For a function of a single variable, we could think about the derivative as the slope of the tangent line. However, for multiple variables, this is not the case. Let’s try to build intuition first by looking at a concrete example! Define the function</p>
<figure data-type="image" tabindex="13"><img src="assets/1590659436-f7ea08b317867c240c365a634905b4d0.png" alt="" loading="lazy"></figure>
<p>which will be our toy example in this section.</p>
<figure data-type="image" tabindex="14"><img src="assets/1590659436-d8b4bbad1dab0ab36d1ad365ef8346d4.png" alt="" loading="lazy"></figure>
<p>Plot of f(x, y)</p>
<p>For functions of two variables, the graph is a surface. We immediately see that the concept of tangent line is not well defined, since we have many lines which are tangent to a given point in the surface. In fact, we have a whole plane of them. This is called the <em>tangent plane</em>.</p>
<figure data-type="image" tabindex="15"><img src="assets/1590659436-96950113d25e9e37c76a8c6317bc6354.png" alt="" loading="lazy"></figure>
<p>Tangent plane for f(x, y) at (0, 0)</p>
<p>However, this tangent plane contains two very special directions. Suppose that we are looking at the tangent plane at <em>(0, 0)</em>. For every multivariable function, fixing all but one variables is basically a function of single variable. In our case, we would have</p>
<figure data-type="image" tabindex="16"><img src="assets/1590659436-1f0b29dce1756add106f303396dd5524.png" alt="" loading="lazy"></figure>
<p>and</p>
<figure data-type="image" tabindex="17"><img src="assets/1590659436-3cec72053e369db388ebe5c224a7c8f9.png" alt="" loading="lazy"></figure>
<p>These functions can be visualized by slicing the surface with a vertical plane perpendicular to one of the axes. Where the plane and the surface meet is the graph of <em>f(x, 0)</em> or <em>f(0, y)</em>, depending on which plane you use.</p>
<figure data-type="image" tabindex="18"><img src="assets/1590659436-fb86b36380f8f7e5f514b827a40c211f.png" alt="" loading="lazy"></figure>
<p>Slicing the surface with a vertical plane to visualize f(0, x)</p>
<p>For these functions we can define the derivatives as we have did in the previous section. These are called <em>partial derivatives</em> and they play an essential role in generalizing our peak finding algorithm from before. To formalize it mathematically, they are defined by</p>
<figure data-type="image" tabindex="19"><img src="assets/1590659436-a57861e002434238f25b172d33ba0495.png" alt="" loading="lazy"></figure>
<p>Each partial derivative represents a direction in our tangent plane.</p>
<figure data-type="image" tabindex="20"><img src="assets/1590659436-78fc7ea2273a258d1f989ea2b02b6ff1.png" alt="" loading="lazy"></figure>
<p>Visualizing the direction of partial derivatives on the tangent plane.</p>
<p>The value of partial derivatives are the slopes of the special tangent lines. The direction of steepest ascent is given by the <em>gradient</em>, which is defined by</p>
<figure data-type="image" tabindex="21"><img src="assets/1590659436-8d907981c80f3774de3520a0b99a5232.png" alt="" loading="lazy"></figure>
<p>Note that the gradient is a direction in the parameter space. The gradients can be visualized in the two dimensional plane easily, which looks like the following in our case.</p>
<figure data-type="image" tabindex="22"><img src="assets/1590659436-1e6a0e03a3d59c535609f74c65918e34.png" alt="" loading="lazy"></figure>
<p>Gradients for f(x, y)</p>
<p>To summarize, the peak finding algorithm is now</p>
<figure data-type="image" tabindex="23"><img src="assets/1590659436-d160fc7defa0aaf7b07064e9d9e18cea.png" alt="" loading="lazy"></figure>
<p>which is called <em>gradient ascent</em>. If we want to find the minimum of a function, we would take a step in the direction of the negative gradient, which is the direction of steepest descent:</p>
<figure data-type="image" tabindex="24"><img src="assets/1590659436-21cb07c11b1e6a30e5003db3bc9f3ac8.png" alt="" loading="lazy"></figure>
<p>This version is called <em>gradient descent</em> and you have probably seen this one more frequently, since in machine learning, we actually want to minimize the loss.</p>
<h2 id="why-does-the-gradient-point-to-the-steepest-ascent">Why does the gradient point to the steepest ascent?</h2>
<p>In this setting, it is not trivial why the gradient gives us the direction of the steepest ascent. To give a precise explanation, we need to do some mathematics. Besides slicing the surface with vertical planes perpendicular to the <em>x</em> or <em>y</em> axis, we can slice it with a vertical plane given by any direction <em>(a, b)</em>. With the partial derivatives, we had</p>
<figure data-type="image" tabindex="25"><img src="assets/1590659436-ce3c02aa38f20d99ea97d92d6278ec39.png" alt="" loading="lazy"></figure>
<p>We can think about these as derivatives of <em>f(x, y)</em> along the directions <em>(1, 0)</em> and <em>(0, 1)</em>. Although these directions are of special significance, we can do this for any direction. Say we have the direction</p>
<figure data-type="image" tabindex="26"><img src="assets/1590659436-ec62eff71d847c43652cb2baeceb7c39.png" alt="" loading="lazy"></figure>
<p>then the <em>directional derivative</em> with respect to this direction is defined by</p>
<figure data-type="image" tabindex="27"><img src="assets/1590659436-4483c2890bb30a5fe01da6407807b0ee.png" alt="" loading="lazy"></figure>
<p>Note that the last identity is nothing else than the dot product (also called scalar or inner product) of the direction vector and the gradient, the same dot product which you have probably encountered in your high school geometry classes. So,</p>
<figure data-type="image" tabindex="28"><img src="assets/1590659436-ea1ce8dad21e54c8e5d600a2803c5d23.png" alt="" loading="lazy"></figure>
<p>The question is the following: which direction maximizes the directional derivative? This would be the direction of the steepest ascent, so if we want to optimize, we want to know this particular direction. To see that this is noting else than the gradient itself as we have mentioned, recall that the dot product can be written as</p>
<figure data-type="image" tabindex="29"><img src="assets/1590659436-c9afe6e5cd84b2cd60f9b43325478955.png" alt="" loading="lazy"></figure>
<p>where |.| denotes the length of a vector and α is the angle between the two vectors. (This is true in arbitrary number of dimensions, not just in two dimensions.) It is easy to see that this expression is maximized when <em>cos α = 1</em>, that is, α is zero. This means that the two vectors are parallel, thus the direction of <em>e</em> must be the same as the gradient.</p>
<h1 id="training-neural-networks">Training neural networks</h1>
<p>Now we are ready to move from theory to practice and see how can we train neural networks. Suppose that our task is to classify images <em>n</em> dimensional feature vectors into <em>c</em> classes. To mathematically formalize our situation, our neural network is represented by the function <em>f</em>, mapping the <em>n</em>-dimensional feature space to the <em>c</em>-dimensional space:</p>
<figure data-type="image" tabindex="30"><img src="assets/1590659436-98bc1f10c851b89edc645a2e831b5fba.png" alt="" loading="lazy"></figure>
<p>The neural network itself is a parametrized function. For notational convenience, we can denote its parameters with a single <em>m</em>-dimensional vector</p>
<figure data-type="image" tabindex="31"><img src="assets/1590659436-3f9b7967e27ee37388447386a368eed6.png" alt="" loading="lazy"></figure>
<p>To explicitly express dependence on parameters, it is customary to write</p>
<figure data-type="image" tabindex="32"><img src="assets/1590659436-62e7273c4a847b000ff0e58300454e06.png" alt="" loading="lazy"></figure>
<p>Training a neural network is equivalent of finding the minimum of the loss function</p>
<figure data-type="image" tabindex="33"><img src="assets/1590659436-41925a8aad0d8a6e4e16ccf64491bf8b.png" alt="" loading="lazy"></figure>
<p>mapping the space of neural network parameters to real numbers. The loss function takes the form</p>
<figure data-type="image" tabindex="34"><img src="assets/1590659436-d70448812895e57f6cbaa4ec2474a616.png" alt="" loading="lazy"></figure>
<p>where</p>
<figure data-type="image" tabindex="35"><img src="assets/1590659436-0cd975e62593bc17874716468f094233.png" alt="" loading="lazy"></figure>
<p>is the <em>i</em>-th data point with observation</p>
<figure data-type="image" tabindex="36"><img src="assets/1590659436-3e2ef135d6ff4a142d7d304ea7d2be6c.png" alt="" loading="lazy"></figure>
<p>and <em>L</em> is the termwise loss function. For instance, if <em>J</em> is the cross-entropy loss, then</p>
<figure data-type="image" tabindex="37"><img src="assets/1590659436-ee5984e8a36b977902fd99d45a9f4f62.png" alt="" loading="lazy"></figure>
<p>where</p>
<figure data-type="image" tabindex="38"><img src="assets/1590659436-0b03c28ec7cbdbe6eeb57b17d9bd30ba.png" alt="" loading="lazy"></figure>
<p>This might seem innocent enough, but it can be really difficult to compute. In real life, the number of data points <em>N</em> can be in the millions, not to say the number of parameters <em>m</em>. So, we have a sum with millions of terms, for which we need to calculate millions of derivatives to minimize. How can we solve this problem in practice?</p>
<h2 id="stochastic-gradient-descent">Stochastic gradient descent</h2>
<p>To use gradient descent, we have to calculate</p>
<figure data-type="image" tabindex="39"><img src="assets/1590659436-c951f51d4c306067cbf4ef5f8d8a7478.png" alt="" loading="lazy"></figure>
<p>which is computationally very intensive if <em>N</em> is large, and <em>N</em> is hopefully <em>very</em> large (because we want lots of data). Can we simplify this? One way to do this is to omit some members of the sum. Although this may sound like an ad-hoc solution, it has solid theoretical foundations. To see this, notice that <em>J</em> can be actually written as an <em>expected value</em>:</p>
<figure data-type="image" tabindex="40"><img src="assets/1590659436-96b60da9c382c5acff80587ce7dd92ee.png" alt="" loading="lazy"></figure>
<p>where</p>
<figure data-type="image" tabindex="41"><img src="assets/1590659436-3640556fbb502e76a03b4c5ce01a05f9.png" alt="" loading="lazy"></figure>
<p>is the (empirical) probability distribution given by our training data. We can treat the sequence</p>
<figure data-type="image" tabindex="42"><img src="assets/1590659436-006796f133223f401aba32ba78858e99.png" alt="" loading="lazy"></figure>
<p>as independent, identically distributed random variables. According to the Law of Large Numbers,</p>
<figure data-type="image" tabindex="43"><img src="assets/1590659436-8ea324d5c8842f419e65bbdc618714c0.png" alt="" loading="lazy"></figure>
<p>holds, where</p>
<figure data-type="image" tabindex="44"><img src="assets/1590659436-6f0635bbdf2bb0fd2b71baba2ba72a9f.png" alt="" loading="lazy"></figure>
<p>is the true underlying distribution. (Which is unknown.) To elaborate more, this means that as we increase our training data, our loss function converges to the true loss. As a consequence, if we subsample our data and only calculate the gradient</p>
<figure data-type="image" tabindex="45"><img src="assets/1590659436-6b9f1d943b36da2b1a687a518ca14d03.png" alt="" loading="lazy"></figure>
<p>for some <em>i</em> instead of all, we still obtain a reasonable estimate if we compute enough. This is called <em>stochastic gradient descent</em> or SGD in short.</p>
<p>In my opinion, there were three fundamental developments which have enabled researchers and data scientists to effectively train deep neural networks: utilizing GPU-s as a general purpose computing tool, backpropagation, and finally stochastic gradient descent. Safe to say that without SGD, wide adoption of deep learning would not have been possible.</p>
<p>As with almost every new approach, SGD also introduces a whole new can of worms. The obvious question is, how large should our subsample size be? Too small size might result in a noisy gradient estimation, while too large has diminishing returns. Selecting the subsample also needs to happen with care. For example if all the subsamples belong to one class, the estimate will probably be off by a mile. However, these issues can be solved in practice by experimentation and proper randomization of the data.</p>
<h1 id="improving-gradient-descent">Improving gradient descent</h1>
<p>Gradient descent (with the SGD variant as well) suffers from several issues which can make them ineffective under some circumstances. For instance, as we have seen, learning rate controls the step size we will take in the direction of the gradient. Generally, we can make two mistakes regarding this parameter. First, we can make the step too large so the loss fails to converge and might even diverge. Second, if the step is too small, we might never arrive to a local minimum, because we go too slow. To demonstrate this issue, let’s take a look at a simple example and study the <em>f(x) = x + sin x</em> function.</p>
<figure data-type="image" tabindex="46"><img src="assets/1590659436-0ba0a1ffce50c91276bcba6fbd5daafe.png" alt="" loading="lazy"></figure>
<p>Suppose that we start the gradient descent from <em>x0 = 2.5</em>, with learning rates α = 1, α = 0.1 and α = 0.01.</p>
<figure data-type="image" tabindex="47"><img src="assets/1590659436-53e9a2fe99d8a7eeeecdcea109415b65.png" alt="" loading="lazy"></figure>
<p>It might not be obvious what is happening here, so let’s plot the <em>x</em>-s for each learning rate.</p>
<figure data-type="image" tabindex="48"><img src="assets/1590659436-f1fa14ff265ea7ec6a5fce1f075a45c9.png" alt="" loading="lazy"></figure>
<p>For α = 1, the sequence is practically oscillating between two points, failing to converge to the local minimum, while for α = 0.01, the convergence seems to be very slow. In our concrete case, α = 0.1 seems just right. How do you determine this in a general setting? There main idea here is that the learning rate does not necessarily have to be constant. Heuristically, if the magnitude of the gradient itself is large, we should reduce the learning rate to avoid jumping too far. On the other hand, if the magnitude is small, it probably means that we are getting close to a local optimum, so to avoid overshooting, the learning rate definitely shouldn’t be increased. Algorithms changing the learning rate dynamically are called <em>adaptive</em>.</p>
<p>One of the most popular examples of such an adaptive algorithm is AdaGrad. It cumulatively stores gradient magnitude and scales the learning rate with respect to that. AdaGrad defines an accumulation variable <em>r0 = 0</em> and updates it with the rule</p>
<figure data-type="image" tabindex="49"><img src="assets/1590659436-2616cabe0e85cfeb13d919def8d4ef6f.png" alt="" loading="lazy"></figure>
<p>where</p>
<figure data-type="image" tabindex="50"><img src="assets/1590659436-f493af5bb2bc75e78f6e980443246817.png" alt="" loading="lazy"></figure>
<p>denotes the componentwise product of two vectors. This is then used to scale the learning rate:</p>
<figure data-type="image" tabindex="51"><img src="assets/1590659436-8a95bfb80883159804f34b3a2a6eaeec.png" alt="" loading="lazy"></figure>
<p>where δ is a small number for numerical stability and the square root is taken componentwise. First, when the gradient is large, the accumulation variable grows rather fast thus decreasing the learning rate. When the parameter is near a local minimum, gradients get smaller so the learning rate decrease practically stops.</p>
<p>Of course, AdaGrad is one possible solution to this problem. More and more advanced optimization algorithms are available every year, solving a wide range of issues related to gradient descent. However, even with the most advanced methods, experimenting with the learning rate and tuning it is very beneficial.</p>
<p>Regarding issues with gradient descent, another is for instance to make sure that we find a global optimum or a local optimum close to it in value. As you can see in the previous example, gradient descent often gets stuck in a bad local optimum. To get a good picture about the solution for this and the other issues, I recommend reading through <a href="https://www.deeplearningbook.org/">Chapter 8 of the Deep Learning textbook</a> by Ian Goodfellow, Yoshua Bengio and Aaron Courville.</p>
<h1 id="how-does-the-loss-function-for-deep-neural-networks-look-like">How does the loss function for deep neural networks look like?</h1>
<p>In our examples during the previous sections, we have only visualized very simple toy examples like <em>f(x) = 25 sin x — x²</em>. There is a reason for this: plotting a function is not straightforward for more than two variables. Since our inherent limitations, we are only able to see and think in at most three dimensions. However, to get a grip on the difficulty of how can the loss function of a neural network look like, we can employ several tricks. One excellent paper about this is <a href="https://arxiv.org/pdf/1712.09913.pdf">Visualizing the Loss Landscape of Neural Nets</a> by Hao Li et al., who were able to visualize the loss function by essentially choosing two random directions and plotting the two-variable function</p>
<figure data-type="image" tabindex="52"><img src="assets/1590659436-a60921e9e9bea6b6edbb9fb08b818636.png" alt="" loading="lazy"></figure>
<p>(To avoid distortions by scale invariance, they also introduced some normalizing factors for the random directions.) Their investigations revealed how skip connections in ResNet architectures shape the loss landscape, making it easier to optimize.</p>
<figure data-type="image" tabindex="53"><img src="assets/1590659436-c72aa6d7b48bd96ff1382aa1c499c905.png" alt="" loading="lazy"></figure>
<p>Source: <a href="https://arxiv.org/pdf/1712.09913.pdf">Visualizing the Loss Landscape of Neural Nets</a> by Hao Li et al.</p>
<p>Regardless of the significant improvement made by skip connections, my point with this was to demonstrate that highly multidimensional optimization is hard. By looking at the first part of the figure, we see that there are many local minima, sharp peaks, plateaus, and so on. Good architecture design can make the job of optimizers easier, but with thoughtful optimization practices we can tackle more complicated loss landscapes. These go hand in hand.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In the previous sections, we have learned the intuition behind gradients and defined them in a mathematically precise way. We seen that for any differentiable function, no matter the number of variables, the gradient always points towards the steepest ascent, which is the foundation of the gradient descent algorithm. Although it is conceptually very simple, it has significant computational difficulties when applied to functions with millions of variables. This problem is alleviated by stochastic gradient descent, however there are many more issues: getting stuck in a local optimum, selecting the learning rate, etc. Because of these, optimization is hard and requires attention from both researchers and practitioners. In fact, there is a very active community out there making it constantly better, with amazing results! After understanding the mathematical foundations of optimization for deep learning, now you are on the right path to improve the state of the art! Some great papers to get you started:</p>
<ul>
<li><a href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape of Neural Nets</a> by Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer and Tom Goldstein</li>
<li><a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> by Diederik P. Kingma and Jimmy Ba</li>
<li><a href="https://arxiv.org/abs/1908.00700v2">Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM</a> by Qianqian Tong, Guannan Liang and Jinbo Bi</li>
<li><a href="https://arxiv.org/abs/1908.03265v1">On the Variance of the Adaptive Learning Rate and Beyond</a> by Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Jiawei Han</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深度学习论文]]></title>
        <id>https://kitenote.github.io/shen-du-xue-xi-lun-wen/</id>
        <link href="https://kitenote.github.io/shen-du-xue-xi-lun-wen/">
        </link>
        <updated>2020-05-24T17:41:19.000Z</updated>
        <content type="html"><![CDATA[<p>Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent networks. In proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8624--8628, 2013.<br>
J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(12):281--305, 2012.<br>
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In proceedings of the International Conference on Artificial Intelligence and Statistics, pages 192--204, 2015.<br>
P. Domingos. A few useful things to know about machine learning. Communications of the ACM, 55(10):78--87, 2012.<br>
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(12):2121--2159, 2011.<br>
I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.<br>
K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In proceedings of the IEEE International Conference on Computer Vision, pages 1026--1034, 2015.<br>
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In proceedings of the International Conference on Machine Learning, pages 448-456, 2015.<br>
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.<br>
F.-F. Li, A. Karpathy, and J. Johnson. CS231n: Convolutional Neural Networks for Visual Recognition. Stanford, 2016.<br>
F.-F. Li, J. Johnson, and S. Yeung. CS231n: Convolutional Neural Networks for Visual Recognition. Stanford, 2017.<br>
A. Ng. Machine learning yearning. Draft, 2016.<br>
A. Ng. CS229: Machine learning. Stanford.<br>
A. Ng. Neural networks and deep learning. deeplearning.ai.<br>
A. Ng. Improving deep neural networks: Hyperparameter tuning, regularization and optimization. deeplearning.ai.<br>
A. Ng. Structuring machine learning projects. deeplearning.ai.<br>
S Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.<br>
N. Srivastava, G. E Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929--1958, 2014.<br>
T. Tieleman and G. Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26--31, 2012.<br>
G. Xavier and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In proceedings of the International Conference on Artificial Intelligence and Statistics, pages 249--256, 2010.<br>
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In proceedings of Advances in neural information processing systems, pages 3320--3328, 2014.<br>
Z.-H. Zhou and J. Feng. Deep forest: Towards an alternative to deep neural networks. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 3553--3559, 2017.<br>
周志华. 机器学习. 清华大学出版社, 2016.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何写好一篇文章？]]></title>
        <id>https://kitenote.github.io/ru-he-xie-hao-yi-pian-wen-zhang/</id>
        <link href="https://kitenote.github.io/ru-he-xie-hao-yi-pian-wen-zhang/">
        </link>
        <updated>2020-05-23T03:51:53.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://pic2.zhimg.com/v2-72a1e736fa7d9bc95fc9f627a902712b_1200x500.jpg" alt="如何写好一篇文章？" loading="lazy"></figure>
<p>转自知乎 <a href="https://www.zhihu.com/people/dai-meng-de-72">戴蒙德</a> <a href="https://zhuanlan.zhihu.com/p/32898022">https://zhuanlan.zhihu.com/p/32898022</a></p>
<p>写作也一年有余，我却时常感到自己的文章水平与一年之前没有质的飞跃。和很多在学习写作的朋友一样，都多少遇到过问题：语言匮乏、缺少素材、词不达意、没有读者，等等。</p>
<p>和那些从小饱读诗书，酷爱文学作品，并且经过系统写作学习的人相比，大部分人缺少机会去接受系统化的写作训练。在自我摸索的过程中，免不了碰壁，确实不知道该如何去写好文章。</p>
<h1 id="一"><strong>（一）</strong></h1>
<p>听人说，写作很简单，是一件人人都学得会的事情。</p>
<p><strong>观察生活、记录事情、抒发感想。</strong></p>
<p>这几个字确实不难，不过真要动起笔来，大多数人就写成了今天吃了什么，看到了什么的流水账。</p>
<p>很羡慕别人能把生活中再简单不过的事情，写的丝丝入扣，让人透过平实的文字也能看到作者的思想。</p>
<p>与其说是写的好，不如说是个人文字运用的能力强。</p>
<p>再伟大的作家，也不是生来就会写的，他们不过是找到了正确的方法，再加上勤奋地练习。</p>
<p>所以，如果我们想要提高自己写作能力，就去有针对性的去学习努力提高这方面的知识技能。</p>
<p>**《写作课》**这本书，就是一本非常好的教人写作的教科书。</p>
<p>本书的作者爱丽斯-马蒂森。她毕业于哈佛大学文学硕士，曾在耶鲁大学、布鲁克林学院教授写作课程。</p>
<p>同时，她也是一名非常优秀的作家，出版了很多小说，并获得过美国最佳短篇小说奖、欧亨利短篇小说奖等。</p>
<p>作为一位在文坛耕耘数十载的小说家，爱丽斯创作出了很多优秀的小说，再加上在顶尖大学教授写作的经历。所以，她将自己多年的写作经验总结在了这本书里。</p>
<h1 id="二"><strong>（二）</strong></h1>
<p>要问一名作者最不能缺的一样东西是什么？</p>
<p>非**“想象力”**莫属了。</p>
<p>但凡是一个善于写作的人，那他也一定是一个善于观察生活，善于联想的人。</p>
<p>还记得《暮光之城》这本小说是怎么来的吗？</p>
<p>作为一部在全世界范围内大受欢迎的纯虚幻的小说，它的诞生源自于作者做的一个梦。她把离奇的梦境加上自己丰富的想象力，写成了这部大热的小说。</p>
<p>作为一名写作者，更善于去捕捉这些突然涌现出来的想法。把那些看起来和我们日常生活没有任何关联的东西用逻辑的链条串起来。</p>
<p><strong>就像工匠一样，把散落的东西一颗一颗缝到一起，串成一串珍珠。</strong></p>
<p>《写作课》这本书主要讲的是小说创作，而小说创作中最核心的部分就是故事。想要写出吸引人的文章，调动读者的感官兴趣，那么一定少不了故事。</p>
<p>这本书的英文名叫<strong>the kite and the string</strong>，直译过来就是**“风筝与线”**。</p>
<p><strong>写作者手中的笔，就像放风筝时手中的线一样，任凭我们的想象力像风筝一样在空中飞升。只要控制得当，那么就能够越飞越高。</strong></p>
<blockquote>
<p>但是，不能无止境的漫步遐想，要按照一定的逻辑和现实基础来进行创作，不可脱离社会常识和基本的逻辑关系。</p>
</blockquote>
<p>我们在日常生活中，总能够发现很多激发写作灵感的瞬间。但是，光靠这些零散的创意并不能写出完整的作品。</p>
<p>作者认为，如果想要真正的掌握文学创作，就要习惯用自己的想象力来填补现实生活的空白。</p>
<p>在创作过程中，不能过于的依赖现实情况。</p>
<p>因为那样写出来的故事过于平淡，根本不会吸引读者注意。因为仅仅是把现实生活中的真人真事换个名字就照搬到文章中来，这样并不能提升我们的写作能力。</p>
<p>只有充分发挥我们的想象力进行虚构，创造出那些在现实生活中不可能发生的事情，这样才能够真正的锻炼写作能力。</p>
<figure data-type="image" tabindex="2"><img src="https://pic2.zhimg.com/80/v2-2bad6aeace72dccca480e37ceadad5c9_720w.jpg" alt="img" loading="lazy"></figure>
<h1 id="三"><strong>（三）</strong></h1>
<p>有了想象力，却还是写不出来，觉得自己缺乏素材，完全不知道该怎么动笔，这也是很多写作者所面临的窘境。</p>
<p>“这个世界不缺少美，缺的只是一双发现美的眼睛。”</p>
<p>灵感不会从天而降，那些写作素材也不会凭空的突然摆在我们面前。我们要主动出击，想要更加有效地收集写作素材，不妨参考一下作者给出的意见。</p>
<p>**1.**试试任意性练习</p>
<p>比如说，某天新闻中提到的事件、</p>
<p>翻开字典第一眼看到的单词，</p>
<p>或是无意中听到的一句话……</p>
<p>那些触发了你的内心的，都可以作为你的写作素材。</p>
<p>**2.**独自思考</p>
<p>如果还是觉得没有灵感，干脆先别管了，静下心来独处一会儿。既然没什么灵感，那就随心所欲的乱写一通，不管有没有用，把脑海中闪过的所有念头都记下来。</p>
<p>**3.**去热闹的地方</p>
<p>再不然的话，就干脆去一个完全陌生的，人流量密集的地方。超市、地铁口就很不错，盯着过往的人看，想想这其中的人，他们身上可能发生哪些故事。</p>
<p>当然，你也可以用自己独特的方法寻找写作素材。</p>
<p>随身揣着一个记事本，看到听到什么事情，都可以记录下来，当你缺乏灵感的时候，不妨翻翻这个小本子，说不定会有意想不到的收获。</p>
<p>让自己习惯于捕捉流动的思想和情感，习惯于观察身边的人或事，并且从中选出最具有吸引力的一个点，作为写作素材。</p>
<h1 id="四"><strong>（四）</strong></h1>
<p>想要写得好，模仿很重要。</p>
<p>我们现在写的大部分文章，都不可能创造出新的文学样式来。</p>
<p>所以，阅读前人的经典著作，无论是对我们的写作风格，还是遣词造句方面，都有非常大的帮助。</p>
<p><strong>我们需要持之以恒的阅读，也不能只读自己喜欢的书。</strong></p>
<p>只有广泛深入的阅读，才能体会到经典作品的精髓。</p>
<p>很多人觉得，要加强写作能力，那直接去买一堆专门讲写作的工具书回来，照着写不就完了。</p>
<p>而作者却指出，这样做也许在短期间能够学会写文章，但学的只是写作的套路。我们不应该为了写作技巧，而阅读写作技巧。</p>
<p><strong>真正的创作是无意的，是积累的结果。</strong></p>
<blockquote>
<p>阅读是为了改变人生，为了培养和完善艺术创作的准确性和完整性。只有这样，才能够创作出真正让自己和别人满意的作品。</p>
</blockquote>
<figure data-type="image" tabindex="3"><img src="https://pic2.zhimg.com/v2-f99c880e9f0cf96e3d175958426affb1_b.webp" alt="img" loading="lazy"></figure>
<h1 id="五"><strong>（五）</strong></h1>
<p>好不容易写完了一篇文章，巴不得马上拿给所有人看。</p>
<p>可是写作的人也都知道，第一稿写出来除了激动人心以外，也让人备受煎熬。因为，要完成一篇不错的文章，需要反复的修改锤炼。</p>
<p><strong>文章不是写出来的，是改出来的。</strong></p>
<p>如果想要有效的修改，就必须尽可能的从陌生人的角度去看作品。试着把自己想象成其他人，把作品放在一边，过一段时间，再去拿出来阅读。</p>
<p>我常常是同时写完几篇文章，然后就放在一边不管。隔一个星期之后，我再拿出来修改。在阅读初稿时，要保持新鲜感，就像第一次读一样。</p>
<p>这个时候，因为没有了刚创作完成时的那种兴奋感，就能更加冷静客观的发现文章中的问题。</p>
<p>最后修改完毕，就可以把自己的作品拿给周围的人去评判，或是发在网上供人浏览人去评价，因为，这样往往可以得到更多客观的评价。</p>
<p>谈到创作，写作必然是一个孤独的过程，创作过程中我们都需要独处，这样才能够触动他人的孤独。</p>
<p>写作，其实并不是一件难事，难的是持之以恒，日复一日的坚持。只要真心想要做这件事，那你根本不用担心自己会灵感枯竭，写不出文章。</p>
<p>大量的阅读经典，充分地发挥想象力，坚持反复的修改，你一定能写出优秀的作品。</p>
<p>转自知乎 <a href="https://www.zhihu.com/people/dai-meng-de-72">戴蒙德</a> <a href="https://zhuanlan.zhihu.com/p/32898022">https://zhuanlan.zhihu.com/p/32898022</a></p>
]]></content>
    </entry>
</feed>